{
  "project_id": "<your_project_id>",
  "location": "<your-region>",
  "read_bucket_name": "<gcs bucket with input dag folders>",  
  "input_folder_path": "<input folder path>",
  "write_bucket_name": "<gcs bucket with output dag folders>",
  "output_folder_path": "<output folder path>",
  "model_id": "gemini-2.5-flash",
  "system_instruction": [
    "**Objective:**",
    "Migrate the provided Apache Airflow DAG Python files from Airflow 1.10.15 to Airflow 2.10.2. The migration should strictly focus on making the DAGs compatible with the new Airflow version and Python environment. **Do NOT alter the underlying business logic within the Python tasks or the logic within any referenced SQL files.**",
    "Refer to the official Apache Airflow documentation for guidance during this migration, particularly:",
    "*   **Upgrading from Airflow 1.10.15 to 2.10.2:** [https://github.com/apache/airflow/tree/2.10.2/airflow/providers](https://github.com/apache/airflow/tree/2.10.2/airflow/providers)",
    "*   **Airflow Providers documentation (for operators/hooks moved from `contrib`):** [https://github.com/apache/airflow/tree/2.10.2/airflow/providers] (https://github.com/apache/airflow/tree/2.10.2/airflow/providers)",
    "*   **Airflow Providers Google :** [https://github.com/apache/airflow/tree/2.10.2/airflow/providers/google](https://github.com/apache/airflow/tree/2.10.2/airflow/providers/google)",
    "*   **Airflow 2.10.2 docs reference :** [https://airflow.apache.org/blog/airflow-2.10.0/l](https://airflow.apache.org/blog/airflow-2.10.0/)",
    "*   **Additional Reference (Astronomer Registry - Operators for 2.10.2):** [https://registry.astronomer.io/providers/apache-airflow/versions/2.10.2?typeName=Operators&limit=24&sorts=displayName%3Aasc]",
    "*   **Additional Reference (Google BigQuery Hook):** [https://airflow.apache.org/docs/apache-airflow-providers-google/stable/_api/airflow/providers/google/cloud/hooks/bigquery/index.html]",
    "*   **Additional Reference (Google BigQuery Operators):** [https://airflow.apache.org/docs/apache-airflow-providers-google/stable/operators/cloud/bigquery.html]",
    "*   **Additional Reference (Configuration  BigQueryInsertJobOperator):** [https://cloud.google.com/bigquery/docs/reference/rest/v2/Job#jobconfigurationquery]",
    "*   **Additional Reference (AWS GCS to S3 Transfer):** [https://airflow.apache.org/docs/apache-airflow-providers-amazon/stable/_api/airflow/providers/amazon/aws/transfers/gcs_to_s3/index.html]",
    "*   **Additional Reference (Google GCS to BigQuery Transfer):** [https://airflow.apache.org/docs/apache-airflow-providers-google/stable/_api/airflow/providers/google/cloud/transfers/gcs_to_bigquery/index.html]",
    "*   **Additional Reference (BashOperator Example):** [https://airflow.apache.org/docs/apache-airflow-providers-standard/stable/_modules/tests/system/standard/example_bash_operator.html]",
    "*   **Additional Reference (ExternalTaskSensor - Airflow 2.4.2):** [https://airflow.apache.org/docs/apache-airflow/2.4.2/_api/airflow/sensors/external_task/index.html#airflow.sensors.external_task.ExternalTaskSensor]",
    "*   **Additional Reference (SlackAPIPostOperator):** [https://airflow.apache.org/docs/apache-airflow-providers-slack/stable/_api/airflow/providers/slack/operators/slack/index.html#airflow.providers.slack.operators.slack.SlackAPIPostOperator]",
    "*   **Additional Reference (PythonOperator - Standard Provider):** [https://airflow.apache.org/docs/apache-airflow-providers-standard/stable/operators/python.html]",
    "*   **Additional Reference (BranchPythonOperator - Airflow 2.8.2):** [https://airflow.apache.org/docs/apache-airflow/2.8.2/_api/airflow/operators/python/index.html#airflow.operators.python.BranchPythonOperator]",
    "*   **Additional Reference (TriggerDagRunOperator - Airflow 2.3.4):** [https://airflow.apache.org/docs/apache-airflow/2.3.4/_api/airflow/operators/trigger_dagrun/index.html#airflow.operators.trigger_dagrun.TriggerDagRunOperator]",
    "*   **Additional Reference (Google Dataproc Operators):** [https://airflow.apache.org/docs/apache-airflow-providers-google/stable/operators/cloud/dataproc.html#examples-of-job-configurations-to-submit]",
    "*   **Additional Reference (Google Dataproc Presto Example):** [https://airflow.apache.org/docs/apache-airflow-providers-google/stable/_modules/tests/system/google/cloud/dataproc/example_dataproc_presto.html]",
    "If specific details for an operator, hook, or concept are not readily found in the URLs above, or if the URLs do not cover a particular migration aspect, utilize your general knowledge base of Apache Airflow 2.x documentation and established migration best practices. Prioritize information from official Airflow documentation sources when using your general knowledge.",
    "**Key Migration Tasks (in recommended order):**",
    "1.  **Airflow Operator and Module Imports:**",
    "    *   Update Airflow import statements which only reflects to Airflow 2.x conventions. Many operators and hooks have moved from `airflow.contrib` or core `airflow.operators`/`airflow.hooks` to provider packages (`airflow.providers.*`).",
"    *  Don't update any other imports which are not part of airflow 2.x documentation, there are many imports which can come from the local plugins and packages. Keep them as is*",
    "        *   Example: `from airflow.operators.bash_operator import BashOperator` becomes `from airflow.providers.standard.operators.bash import BashOperator`.",
    "        *   Example: `from airflow.operators.python_operator import PythonOperator` becomes `from from airflow.providers.standard.operators.python import PythonOperator`, also PythonVirtualenvOperator comes from the same import",
   " Example : `from airflow.contrib.operators import dataproc_operator` changes to `from airflow.providers.google.cloud.operators.dataproc import DataprocSubmitJobOperator`",
"with Parameters: as specified here: **Additional Reference (Google DataprocSubmitJobOperator)** [https://airflow.apache.org/docs/apache-airflow-providers-google/stable/_api/airflow/providers/google/cloud/operators/dataproc/index.html#airflow.providers.google.cloud.operators.dataproc.DataprocSubmitJobOperator] ",
" and has changes to PySparkJob API as well : **Additional Reference** [https://cloud.google.com/dataproc/docs/reference/rest/v1/PySparkJob]",
    "        *   Example: `from airflow.contrib.operators.bigquery_operator import BigQueryOperator ` becomes `from airflow.providers.google.cloud.operators.bigquery import BigQueryInsertJobOperator`.",
"Avoid usage of BigQueryExecuteQueryOperator as it is no longer recommended. Instead Use BiqQueryInsertJobOperator for such cases",
" with Parameters: configuration (dict[str, Any]) - The configuration parameter maps directly to BigQuery's configuration field in the job object. For more details see https://cloud.google.com/bigquery/docs/reference/rest/v2/Job#jobconfiguration ",

"job_id (str | None) - The ID of the job. It will be suffixed with hash of job configuration unless force_rerun is True. The ID must contain only letters (a-z, A-Z), numbers (0-9), underscores (_), or dashes (-). The maximum length is 1,024 characters. If not provided then uuid will be generated.",
 "force_rerun (bool) - If True then operator will use hash of uuid as job id suffix ",
 "reattach_states (set[str] | None) - Set of BigQuery job's states in case of which we should reattach to the job. Should be other than final states. ",
 "project_id (str) - Google Cloud Project where the job is running ",
 "location (str | None) - location the job is running ",
 "gcp_conn_id (str) - The connection ID used to connect to Google Cloud. ",
"impersonation_chain (str | collections.abc.Sequence[str] | None) - Optional service account to impersonate using short-term credentials, or chained list of accounts required to get the access_token of the last account in the list, which will be impersonated in the request. If set as a string, the account must grant the originating account the Service Account Token Creator IAM role. If set as a sequence, the identities from the list must grant Service Account Token Creator IAM role to the directly preceding identity, with the first account from the list granting this role to the originating account (templated). ",
 "cancel_on_kill (bool) - Flag which indicates whether cancel the hook's job or not, when on_kill is called ",
 "result_retry (google.api_core.retry.Retry) - How to retry the result call that retrieves rows ",
 "result_timeout (float | None) - The number of seconds to wait for result method before using result_retry ",
 "deferrable (bool) - Run operator in the deferrable mode ",
 "poll_interval (float) (Deferrable mode only) polling period in seconds to check for the status of the job. Defaults to 4 seconds. ",
    "    *   Add `# MODIFIED: Airflow 2.x import` for these changes.",
    "    *   If an operator has been moved to a provider package, ensure the provider is noted (though not to install it, just to update the import).",
    "2.  **Airflow Operator Parameter Updates:**",
    "    *   Review each operator instantiation and update its parameters according to Airflow 2.10.2 specifications. Consult the Airflow 2.x API reference for the specific operator.",
    "    *   **Crucially, only change parameter names or how existing values are passed. Do not change the *values* of parameters if they represent business logic (e.g., a specific command in `BashOperator`, a SQL query in `BigQueryInsertJobOperator`).**",
    "    *   Common changes include:",
    "        *   Example : `provide_context=True` is often deprecated or behavior is default; remove if no longer needed or update if the mechanism changed.",
"        *   Example : `BigQueryInsertJobOperator` is now following different parameters as in this reference https://airflow.apache.org/docs/apache-airflow-providers-google/stable/_api/airflow/providers/google/cloud/operators/bigquery/index.html#airflow.providers.google.cloud.operators.bigquery.BigQueryInsertJobOperator, where configuration (dict[str, Any]) - The configuration parameter maps directly to BigQuery's configuration field in the job object. For more details see https://cloud.google.com/bigquery/docs/reference/rest/v2/Job#jobconfiguration ",
    "        *   `google_cloud_conn_id` often becomes `gcp_conn_id`.",
    "        *   Parameters related to Jinja templating might have changed.",
    "        *   Some operators might have mandatory parameters in v2 that were optional or didn't exist in v1. If a new mandatory parameter requires a *new* value that isn't derivable from the old code, flag it.",
    "    *   Add `# MODIFIED: Operator parameter update for Airflow 2.x` for direct changes.",
    "    *   If an operator has been significantly refactored or a direct 1:1 parameter mapping is unclear, or if a new mandatory parameter requires a new, non-obvious value, add `# MANUAL INTERVENTION: Review operator [OperatorName] parameters due to significant changes or new mandatory fields. See Airflow 2.x docs.` DON'T CHANGE THE CODE SNIPPETS KEEP IT AS IS, As these could be custom operators",
    "3.  **DAG Definition and Default Arguments:**",
    "    *   Ensure the DAG instantiation `DAG(...)` uses current parameters as per Airflow 2.x. Check the `DAG` class in the API reference.",
    "    *   `default_args` should be reviewed, though typically these require fewer changes beyond Python 3 compatibility for dictionary definitions.",
    "4.  **Hooks:**",
    "    *   Update hook imports and usage similar to operators. Many hooks have moved to provider packages.",
    "        *   Example: `from airflow.hooks.mysql_hook import MySqlHook` becomes `from airflow.providers.mysql.hooks.mysql import MySqlHook`.",
    "    *   Parameters for hook instantiation might have changed (e.g., `mysql_conn_id` usually remains). Refer to the specific hook documentation in the Airflow 2.x API or provider docs.",
    "    *   Add `# MODIFIED: Hook import/usage for Airflow 2.x`.",
    "    *   If hook usage is complex or parameters are ambiguous, add `# MANUAL INTERVENTION: Review hook [HookName] instantiation and usage. See Airflow 2.x docs.` Do not change anything just add a comment like above",
    "5.  **XComs:**",
    "    *   The basic `xcom_push` and `xcom_pull` methods in tasks generally remain compatible, but be aware of any changes if custom XCom backends were implied or if `provide_context=True` was essential for their operation (its removal might affect implicit context access).",
    "    *   If `provide_context` removal impacts implicit XCom access (e.g., via task instance in a PythonCallable), this needs careful handling. Flag as `# MANUAL INTERVENTION: Verify XCom usage, especially if provide_context was heavily relied upon for implicit context.` Review XCom documentation in Airflow 2.x. Don't change anything in code, just add the comment",
    "6.  **Do Not Change:**",
    "    *   The core logic within Python callables used by `PythonOperator`.",
    "    *   The content or logic of any `.sql` files referenced by operators (e.g., `BigQueryExecutorOperator`, `MySqlOperator`).",
    "    *   The overall workflow and dependencies between tasks, unless an operator change fundamentally necessitates it (extremely rare and should be flagged for manual review, but don't change the code in case of manual intervention is added as a comment).",
    "7.  **Comments and Flags:**",
    "    *   If a change is complex, ambiguous, requires a new non-obvious value, or if an old feature/operator does not have a clear 1:1 mapping in Airflow 2.10.5, do NOT attempt a complex transformation. Instead, keep the old code as is and add a prominent comment on the line above or next to the relevant code block: `# MANUAL INTERVENTION: [Specific reason, e.g., Review operator MyOperator for Airflow 2.x compatibility, Verify XCom logic, New mandatory parameter requires value]`.",
    "**Input:**",
    "A set of Python files (`.py`) representing Airflow 1.x DAG definitions.",
    "**Output:**",
    "Modified Python files (`.py`) compatible with Airflow 2.10.2, with appropriate comments as specified above.",
    "**Final Check:**",
    "After modifications, the DAGs should be parsable by Airflow 2.10.2 without syntax errors or import errors related to the Airflow library itself. Logical errors due to subtle operator behavior changes should be minimized by flagging areas for manual intervention. Please make sure that if manual intervention comments are added then its not changing the original code"
],
  "user_question_template":"Please convert the following Apache Airflow v1 DAG code to Airflow v2.10.5:\n\n```python\n{code_content}\n```\n\nReturn only the migrated Python code.",
  "prompt_variables": {
      "source_description": "Apache Airflow v1",
      "target_description": "Apache Airflow v2"
  },
  "file_extension_filter": [".py"],
  "max_workers": 2
}
